\begin{center}
\chapter{Training the Convolutional Neural Network}
\end{center}
For our pipeline to predict what type a particular sign is, it requires a classifier. For this task, we have chosen to implement and train a convolutional neural network (also known as CNN or convnet).
In this chapter, we will briefly discuss what is a convolutional neural network and then present our way of applying it to pictures of traffic signs, detailing and explaining the choices in hyperparameters and learning strategies.

\section{What is a CNN?}
Convolutional Neural Networks, as defined by LeCun et al.\cite{cnnOriginal} is a type of feed-forward neural network inspired from biology in that the connection between neurons closely resembles that of the animal visual cortex. They are mostly used in machine learning for visual recognition tasks or natural language processing, and offer a suite of benefits over their counterparts, such as shift and translation invariance.
They are similar to regular feed-forward neural networks in that they have a fixed input size, multiple hidden layers and an output layer which performs classification. However, the connections between neurons are local (a neuron in a layer is only connected to a small region of the previous layer), allowing for greater scalability when dealing with images. Next we will present the layers specific to this type of neural network.
\subsection{Convolutional Layers}
Images can be represented as 3-dimensional arrays defined as $height*width*depth$ (where depth is the number of color channels). A convolutional layer simply receives an input volume and passes on an output volume. It is defined by four parameters: width, height, number of filters and stride. During the forward pass, each filter is slided across the input volume (over width and height) and a dot product is computed between the values of the filter and the input volume's patch. Each dot product generates a two-dimensional activation map representing the response of that filter for that particular region. By stacking each resulted activation map along the $depth$ dimension, we produce an output volume. Intuitively, this process increases the depth of the volume, but reduces the height and width.
\includegraphics[width=1.0\linewidth]{nn}
\captionof{figure}{Left: Regular neural network. Right: Convolutional Neural Network with several convolutional layers stacked together.}

Each filter has its own set of weights that are updated (learned) when running backpropagation.
Note that a fully-connected neural network would require several orders of magnitude more connections between layers causing it to not scale well (e.g. for a $30*30*3$ image, each neuron in the first hidden layer would have to have 2700 weights. Adding up the weights for each neuron in the hidden layer, this number gets out of control rapidly).
\subsection{Pooling Layers}
Pooling layers have a similar purpose to that of the convolutional layers, in that they reduce the spatial size of the input's representation. They are defined by three parameters: height, width and stride. They work by sliding a $h*w$ window over the input volume, independently for every depth slice, and applying a single-output operation over the window. More generally, they produce an output volume $W_2*H_2*D_2$ defined by

\begin{gather*}
W_2=\frac{W_1-w}{S+1}\\
H_2=\frac{H_1-h}{S+1}\\
D_2=D_1
\end{gather*}
where $W_1*H_1*D_1$ is the input volume, $S$ is the stride and $w,h$ are the width and height of the sliding window.\\
Note that the depth width and height of the resulting volume are reduced, and the depth remains unchanged.\\
Most common operations used when pooling are MAX, MIN and AVG. In our final architecture, we have chosen the MAX function, hence the layer's name Max Pooling. A layer of pooling is typically added after every convolution.

\section{Architecture}
We defined a network with three sequences of convolution and max pooling layers, followed by a flattening layer (so that the output is compatible with the following layers), dropout and two fully connected layers, the last one being responsible for outputing the classification. The network architecture is summarised in the table below (generated by calling the \textit{model.summary()} method). Before being passed to the network, images are resized to $32*32$ pixels each. Note that activation, dropout and flattening layers have not been included for the sake of brevity.
\begin{center}
 \begin{tabular}{||c | c | c | c||} 
 \hline
 Layer (type) & Output Shape &  No. of params \\ [0.5ex] 
 \hline\hline
 Conv2D-1 & (30,30,32) & 896 \\ 
 \hline
 MaxPool2D-1 & (15,15,32) & 0 \\
 \hline
 Conv2D-2 & (13,13,64) & 18496\\
 \hline
 MaxPool2D-2 & (6,6,64) & 0\\
 \hline
 Conv2D-3 & (4,4,128) & 73856\\ [1ex] 
 \hline
 MaxPool2D-3 & (2,2,128) & 0\\
 \hline
 Dense-1 & (1024) & 525312\\
 \hline
 Dense-2 & (43) & 44075\\
 \hline\hline
\end{tabular}
\end{center}

The final dense layer has 43 outputs representing class probabilities for each type of sign. $argmax$ is performed over them, and the class with the highest probability is chosen as the classification.\\
Each convolutional layer uses $1*1$ strides, while the max pooling layers use a $3*3$ stride. The number of layers depends on the input resolution; in our case, resizing images to a lower resolution would cause a considerable loss of quality, and increasing the resolution only adds to training time without gaining any boost in accuracy (in some cases, even hurting it), so a three layer approach was chosen, with the output volumes being condensed to $(2,2,128)$ in the final layer. \\
Total trainable parameters for this network is 662.635, which is manageable, perhaps even on the low side considering current trends of deep networks. Another gain of having a relatively shallow network is better inference time.
\section{Initialization Strategy}
By initializing all the weights and biases of the neural network to zero, we will cause them to not be updated in future iterations. This is explained by how the backpropagation algorithm works.
\begin{align}
\frac{\delta}{\delta\theta_{ij}^l}=\alpha_j^l\delta_i^{l+1}\\
\alpha_j^l=g(\theta^{l-1}\alpha^{l-1})\\
\delta^l=(\theta^l)^T\delta^{l+1}*g'(\theta^{l-1}\alpha^{l-1})
\end{align} 
where \\
\tab(2.1) is the calculation of the partial derivatives for weights in layer l\\
\tab(2.2) the application of non-linearity $g$ to the neuron's output\\
\tab(2.3) the difference propagated backwards from the succesive layer\\
and $*$ stands for element-wise multiplication.\\
By looking at how the weights are computed, zero-valued weiths would prevent the activation from changing. For $\delta$, zero-valued weights multiplied by the delta from the succesive layer's delta lead to zero delta, effectively blocking the errors from propagating past them. Note that choosing another constant value for initialization would lead to the network getting stuck in a local minima in the loss function's hyperspace.\\
Therefore, an initialization strategy based on random sampling is required. For this we have chosen the "Glorot Uniform" method, as described in the paper by Xavier Glorot\cite{glorotPaper}. It works by drawing a sample from a uniform distribution within $[-a,+a]$ where $a=\sqrt{\frac{6}{n_{inputs}+n_{outputs}}}$ with $n_{inputs}$ the number of inputs in the weight tensor and $n_{output}$ the number of output units in the weight tensor.\\
This initialization strategy is also a good way of preventing the vanishing/exploding gradients problem. It ensures that the signal flows properly in both directions, without it dying out or exploding and saturating. The authors of this method argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and the gradients have to have equal variance before and after flowing through a layer in the reverse direction. It is actually not possible to guarantee both unless the layer has an equal number of input and output connection, but the initialization scheme described above is a good compromise.\\
Note that we have mentioned earlier in this paper that randomization should be turned off when validating for different hyperparameter values. However, when dealing with weight initialization, it is simply unavoidable and some margin of error has to be taken into account.

\section{Activation Functions}
Four activation functions are compared in this section: logistic (sigmoid), ReLU (rectified linear unit), leaky ReLU and ELU (exponential linear unit). For the final layer, the softmax activation is detailed immediately afterwards.\\
\includegraphics[width=\linewidth]{activations}
\captionof{figure}{Plotting the four activation functions we have tested.}
\begin{center}
$sigmoid(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x+1}$\\
\end{center}

The \textbf{sigmoid function}  is the first function we have tested and has caused training to be slower than expected. It is also known to cause other problems such as the vanishing gradients (due to the fact that the function saturates at 0 or 1, its derivative getting extremely close to 0, causing the gradients to progressively get diluted as backpropagation passes them down to the lower layers), particularly for deeper networks. As such, we have chosen to avoid using the sigmoid activation, even though it is standard in many networks.\\

\begin{center}
$relu(x) = max(0,x)$
\end{center}
\textbf{ReLU} (rectified linear unit) is a slight improvement in the sense that it does not saturate for positive values, but has shown similar computation speeds and caused us to run into the \textit{dying ReLUs} problem, especially when we used a larger learning rate. During training, if a neuron's weights get updated such that the weighted sum of the neuron's inputs are equal to 0, it will start outputting only 0 (the neuron has "died"). In this event, it is unlikely that the neuron will "come back to life" since the gradient of the ReLU function is 0 when its input is negative. To circumvent this problem, we have used some of the variants of ReLU.
\begin{center}
$leaky\_relu_\alpha(x)= 
	\begin{cases}
		x &\quad x \geq 0\\
		x\alpha & \quad x < 0\\
	\end{cases}
$
\end{center}
\textbf{Leaky ReLU} allows for a small slope when $x<0$. It has outperformed the simple ReLU function in terms of performance and has completely eliminated the dying ReLUs problem. We have tested with multiple values for the $\alpha$ parameter, ranging from 0.01 to 0.2, and found comparable performances.

\begin{center}
$elu(x)=
	\begin{cases}
		x &\quad x \geq 0\\
		\alpha(\exp(x)-1) &\quad x < 0
	\end{cases}
$
\end{center}
\textbf{ELU} has shown to outperform all other variants of the ReLU function in terms of accuracy. It has also boosted training time by a considerable amount (in the sense that it took less epochs to reach similar performance) due to the fact that the exponential linear units try to make the mean activations closer to zero. Given this result, it has been chosen as the activation function for the convolutional and fully-connected layers.\\

For the final layer of the neural network (which is responsible for classification) we have used the \textbf{softmax} activation,  a generalized form of the logistic function, which transforms a real-valued k-dimensional vector $z$ to a k-dimensional vector $\sigma (z)$ of real values, where each entry is in the interval $(0,1)$ and all the entries add up to 1. It is defined as:
\begin{center}
$\sigma (z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} $ for $ j = \overline{1,K}$\\
with $K$ the total number of classes (in our case, 43)
Its properties are very useful since we want our model to output a probability attached to each class.
\end{center}
\section{Optimizers}
Trying to improve on the performance of the standard stochastic gradient descent, we have experimented with several other optimization algorithms. We have tested Nesterov Accelerated Gradient, Root-Mean-Square Propagation (RMSProp) and Adaptive Moment Estimation (Adam). In this section we will briefly look over at how these optimizers work and compare their resuls.\\
\textbf{Nesterov Accelerated Gradient} proposed by Yurii Nesterov\cite{nesterov} is a variant of the momentum optimization technique. The idea behind it is to measure the gradient of the cost function not at the local position, but one step ahead in the direction of the momentum.\\
\begin{gather}
m \leftarrow \beta m + \eta \nabla_\theta J(\theta+\beta m) \\
\theta \leftarrow \theta - m
\end{gather}
where $\beta \in [0,1]$ is the momentum, J the cost function, $\eta$ the learning rate, m the momentum vector, $\theta$ the weights, and $\nabla_\theta J(\theta+\beta m)$ the gradient of the cost function measured at the direction ahead of the current gradient.\\
This tweak works because, in general, the momentum vector will generally be pointing in the right direction, so it will be slightly more accurate to use the gradient measured a bit farther in that direction. As evidence, it has outperformed regular stochastic gradient descent by a large margin. The hyperparameter $\beta$ can be seen as a friction factor to prevent the momentum from getting out of hand; 0 means high friction, and 1 means no friction. A typical momentum value is 0.9, which we have used in our tests.

\textbf{Root Mean Square Propagation} has outperformed the Nesterov optimizer by approx. 4 percentage points in terms of accuracy when running on 60 epochs, and offered an improved loss value. It works by scaling down the gradient vector along the steepest direction.
\begin{gather}
s \leftarrow \beta s + (1 - \beta) \nabla_\theta J(\theta) \otimes \nabla_\theta J(\theta) \\
\theta \leftarrow \theta - \eta \nabla_\theta J(\theta) \div \sqrt{s+\epsilon}
\end{gather}
The first step accumulates the square of the gradients from the most recent iterations (by using the exponential decay rate $\beta$, typically set to 0.9). \\
The second step is similar to Gradient Descent, except it scales down the gradient vector by a factor of $\sqrt{s+\epsilon}$, where $\epsilon$ is a smoothing term to avoid division by zero, typically set to $10^{-10}$.\\
In short, this optimizer decays the learning rate, but it does so faster for steep dimensions and slower for dimensions with gentler slopes. Its adaptive learning rate is very useful since it means we don't have to tune the $\eta$ parameter as much.\\
\textbf{Adam} (Adaptive Moment Estimation) is the final optimizer we have experimented with, and has shown very similar performance with RMSProp ($\pm{1\%}$). However, the training time has been reduced, and, as such, it has been chosen as the optimizer for our neural network. It keeps track of an exponentially decaying average of past gradients, and an exponentially decaying average of past squared gradients. It is described as follows:
\begin{gather}
m \leftarrow \beta_1 m + (1-\beta_1) \nabla_\theta J(\theta) \\
s \leftarrow \beta_2 s + (1-\beta_2) \nabla_\theta J(\theta) \otimes \nabla_\theta J(\theta)\\
\theta \leftarrow \theta - \eta m \div \sqrt{s + \epsilon}
\end{gather}
The momentum decay parameter $\beta_1$ is typically chosen 0.9, and the scaling decay parameter $\beta_2$ is initialized to 0.999. As earlier, the smoothing term is chosen as $10^{-10}$.
\begin{center}
\includegraphics[scale=0.5]{val_loss_optimizers}
\captionof{figure}{Plotting the validation loss of the four optimizers for 60 epochs of training.}
\end{center}
\section{Key Performance Indicators}
In order to evaluate the performance of our network, we used a loss function and an accuracy metric.\\
For loss function we have used categorical crossentropy, which models the difference between the ground-truth distribution y and the predicted distribution $\hat{y}$. Mathematically, it is described as follows:\\
\begin{center}
$$H(y,\hat{y}) = - \sum_i y_i \log \hat{y}_i$$
\end{center}
Backpropagation minimizes the loss function with respect to the network's weights. In doing so, it gets as close as possible to the ground-truth in hopes that it can generalize to novel data.\\
To measure how many of our samples are predicted correctly, we have used the accuracy metric, which is the ratio between the correctly predicted samples and the total number of samples.(true positives and true negatives divided by the number of samples)
\begin{center}
$acc(X,y) = \frac{tp+tn}{tp+tn+fp+fn}$
\end{center}
Note that when we have discriminated between hyperparameter settings we have used these metrics on the validation and training data. These metrics were used on the test data only when a good baseline was established for the model.
\section{Regularization}
We have observed that our network began overfitting the training set without achieving an acceptable performance. As such, we had to employ some methods to regularize it in order to avoid this problem.\\
\textbf{Dropout} works by assigning a dropout rate $p$ to each neuron in a layer. At every training step, that neuron will be "dropped out" with $p$ probability, effectively being ignored for that respective step. This works due to the fact that the neuron is unable to co-adapt with its neighboring neurons and has to learn by itself, ending up being less sensitive to changes in inputs. A rate of 0.3 has been used for the fully-connected layer. Adding dropout to convolutional layers seemed to decrease performance and is generally not a good practice. Adding a larger degree of dropout would cause the network to underfit the dataset.\\
\textbf{L2-regularization} works by tweaking the loss function to include the neurons' weights. Specifically, our loss function becomes:
\begin{center}
$$H(y,\hat{y}) = - \sum_i y_i \log \hat{y}_i + \lambda \sum_{i} w_i^2$$
where the term $$\lambda \sum_{i} w_i^2$$ is the sum of the squared weights of all neurons to which we applied regularization, and $\lambda$ is a hyperparameter.
\end{center}
The effect of the regularization is that our loss function will yield worse values for bigger weights. In doing so, the network is forced to keep the weights of the regularized layers on the small side.\\
We applied regularization to the first fully connected layer (with a standard $\lambda=1e-4$) and have observed more robustness in our network and resistance to overfitting. Adding it to convolutional layers has incapacitated the feature extraction ability of the network and as a result the performance dropped significantly; therefore, we have chosen not to use it on the convolutional layers.
\section{Other Parameters}
\subsection{Learning Rate \& Scheduling}
Since we are using an adaptive learning rate algorithm (Adam) it is not strictly necessary to fine tune the learning rate. 		Adam's default learning rate is $1\mathrm{e}{-3}$ and, in practice, yields good results. However, we have found out that when 	coupling it with a learning schedule and using a more aggresive learning rate we obtain several benefits, such as training for a 	larger number of epochs before overfitting and reaching the performance of the model with no schedule faster (in a lower number 	of epochs). \\
The particular schedule that yielded the best results was monitoring the value of the loss function and reducing the learning 	rate by a factor of $20\%$ once there is no improvement for 3 epochs. A minimum threshold of $1\mathrm{e}{-4}$ for the learning rate is applied.\\
Adding a learning schedule to an already adaptive learning rate optimizer is currently an \textbf{undocumented} method, and is one of the novel insights generated through our experiments. The final learning rate set for the Adam optimizer is $3\mathrm{e}{-3}$.\\
\begin{center}
\includegraphics[scale=0.5]{lr-decay}
\captionof{figure}{Learning rate as decayed by the scheduler.(300 epochs)}
\end{center}
\subsection{Model Checkpoint \& Epochs}
We have added two model checkpoint callbacks which monitor the value of the loss function as well as the accuracy on the validation data, and save the best performing models, depending on each metric. This was done to ensure that we can use a higher number of training epochs without overfitting, allowing us to save compute time. As such, the total number of epochs trained on was 300, with overiftting observed around epoch 280, where the performance on the training and validation set began to diverge.
\subsection{Batches}
In all of the experiments we have used mini-batch training. The difficulty in choosing the number of samples to include in each mini-batch comes from the fact that using a lower number will, on the one hand, require much less memory and train faster, but on the other hand using a number of samples that is too small might affect the accuracy of the gradient estimates. Standard batch sizes that offer compromises between the two extremes are $128,256,512,1024,2048$. In our case, a batch larger than $256$ seemed to yield no noticeable benefits, but a batch of $128$ damaged the model's performance. As such, we have chosen $256$ as the size of the batch for our model.

\section{Putting It All Together}
Until now, we have detailed the choices in the network's hyperparameters. In this section, we will present our results. Below is a chart for the training and validation values of the performance indicators.
\begin{center}
\includegraphics[scale=0.5]{train_val}
\captionof{figure}{Training and validation metrics functions.}
\end{center}
The difference in the values of the training and validation sets are due to the fact that the validation set has a much lower number of samples than the training set and does not have enough representation power when compared. However, it is still useful as a metric since their trends should be similar (upward for accuracy, downward for loss) before overfitting. At around epoch 270, barely any change is noticeable, and the network starts to overfit. The model recorded at around that epoch is stored and saved.\\
Test set results are $\textbf{98.1\%}$ accuracy and a loss value of approx. $\textbf{0.116}$, which is quite good considering that human-level detection is, according to a recent paper by Yann LeCun\cite{lecunDetection} approximately the same: $98.8\%$.