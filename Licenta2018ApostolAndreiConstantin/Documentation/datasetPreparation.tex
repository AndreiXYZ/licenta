\begin{center}
	\chapter{Dataset Preparation}
\end{center}

The dataset used for training the convolutional neural network is the German Traffic Sign Recognition Benchmark.\cite{GTSRB}
This dataset consists of 43 classes of traffic signs, with one directory per class. It contains 26,640 training images and 12,569 testing images. Each image contains one traffic sign each with approximately 10\% border around it. The traffic sign is not necessarily centered within the image. Sizes of each image may vary, from 15x15 up to 250x250.\\
Each sign has a class index, ranging from 0 to 42. Training images are grouped by tracks. Each track contains 30 images of one single physical traffic sign in various lighting conditions, angles and distances. We will need to take this fact into account when building our validation set.
Here are a few samples from the dataset:
\begin{center}
\includegraphics[width=0.6\linewidth]{gtsrb-sample}
\captionof{figure}{Samples from the training set.}
\end{center}
The dataset is already partitioned with approximately 70\% of the total images in the training set and the remainder of 30\% into the test set. 
The images come in the PPM (Portable PixMap) format and are stored in memory in the form of $H*W*3$ arrays, where H is the height of the picture, and W is the width, each with 3 channels of colors in the RGB colorspace.

\section{Balancing}
A balanced dataset is one where each class has roughly the same number of points. While this is ideal, in practice it is rarely the case. Many machine learning models, including neural networks, tend to learn more from overrepresented classes. As such, a skewed or unbalanced distribution can affect the model's performance and ability to generalize. By plotting the number of samples in each class as a bar chart (class index on x-axis, number of samples on y-axis), we obtain the following figure:\\
\begin{center}
\includegraphics[width=0.6\linewidth]{GTSDB-Training-Img-Distribution}
\captionof{figure}{Distribution of samples per class}
\end{center}
The training set seems to be severely unbalanced, with some classes having 10 times more samples than the most underrepresented class. This can cause our model to learn more about the overrepresented classes since more gradient descent updates are performed, tilting its parameters in a direction that favours those particular signs, thus being unable to create an internal representation of those with a low number of samples. 
Unbalanced datasets are a relatively common problem in machine learning, so several solutions have been proposed in official literature. We have chosen a method called class weighting. The first step is to compute each class' inverse ratio relative to the class with the highest number of samples (also called reference class):\\
\begin{gather*}
	m = \max\limits_{0 \leq i \leq 42} (|s_i|)\\
	r_i =  \frac{m}{|s_i|},\\
\end{gather*}
\begin{center}
	where $s_i$ is the set representing the data points in class $i$.
\end{center}	
We can then use each class' ratio as defined above to weight the neural network's loss function during training, effectively causing the model to perform gradient descent updates proportional to the ratio. This means that for classes with a small number of data points, updates are larger. For the reference class, the values of the updates do not change, since the ratio is 1. This is mathematically equivalent to feeding duplicates from the smaller classes into the network.\\
Note that there is no need to balance the test and validation sets since they play no part in the learning process itself, and are only used for benchmarking.
\section{Building Validation Set}
Validation sets are used to estimate how well a model has been trained. Specifically, we can use it to train several models using different hyperparameters and/or learning strategies and compare their results. Once we have chosen the best performing model, we can check it against the test set. This is done to ensure that the choice of parameters is as unbiased as possible;  choosing a model for its results on the test set does not necessarily imply its ability to generalize well.\\
Since the images in our dataset are grouped by tracks, we cannot include pictures from the same track into the training and the validation set, as it would damage the quality of our key performance indexes (our model would be tested on images that are too similar to the ones it has been trained on). Therefore, two full tracks have been randomly selected from each class and have been moved into the validation set. This number has been chosen as to keep the quality of our initial dataset (some classes only have five tracks) and build a sufficiently large validation set for the purpose of testing. \\
By doing this, we build a set with 1,290 images, which we have used when exploring how the choice of some hyperparameters affects our model's performance, and also as a metric for creating checkpoints. This process is explained in-depth in the $2^{nd}$ chapter.

\section{Augmentation}
Although the pictures in our dataset are taken at varying distances, lighting conditions and/or angles, it is not enough for our system to generalise. Also, the border around the signs is rather small (around 10\% of the image's dimension), allowing only for a thin margin of error when used in conjunction with a detection system. As such, more variance is needed in our pictures. For this task, two main methods have been proven to work best: getting more samples (in our case, collecting more pictures of traffics signs), and artificially inflating our existing set (also known as augmentation).\\
Increasing the size of our dataset by collecting more images causes our distribution to be more representative, and variation occurs naturally. However, this process is very costly and, in some cases, not even possible. As such, we have opted for the second option, augmentation.\\
We augment our images by using an ImageDataGenerator object provided through the Keras API. We define it as follows:
\begin{lstlisting}[language=Python]
from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
          rescale=1./255,
          rotation_range=10,
          width_shift_range=0.2,
          height_shift_range=0.2,
          shear_range=0.2,
          zoom_range=0.2,
          fill_mode='nearest')
\end{lstlisting}
The ImageDataGenerator object defines a set of transformations to be applied to each image during the training process. A random float is chosen for each category defined by us, in the $[-x,+x]$ range, where x is the maximum transformation intensity set by us.\\
In the particular object defined above, images are shifted by height and width, zoomed into, and sheared by up to 20\% each (values chosen independently). The rotation range is $\pm{10}$ degrees. Rescaling by $1./255$ means dividing the intensity of each pixel by 255, squashing their values in the $[0,1]$ range, effectively turning the image to grayscale. During runtime, each parameter is chosen randomly and independently, and the transformation is applied to it before being passed on, so the neural network does not see the same image twice. Here is a preview of a set of transformations applied to a single image:

\begin{center}
\includegraphics[width=0.2\linewidth]{original}
\captionof{figure}{Original image.}
\includegraphics[width=0.75\linewidth]{augmentation_preview}
\captionof{figure}{Several possible transformations applied to the image.(rescaling not included)}
\end{center}
The process of augmentation can be seen as a form of regularization (the network's internal representation has to account for a higher degree of variance in the samples). The exact values when defining the generator have been chosen as a middle ground between adding a negligible amount of variance and regularizing the model to the point where its performance is affected. Rescaling (converting to grayscale) has shown, counter to intuition, an improvement in overall accuracy, and is also useful in keeping the numbers (e.g. variables in the backpropagation algorithm) on the low side.
\\
Note that augmentation has to be turned off when performing hyperparameter optimization since we want to have as little randomness as possible when running our experiments, and only use it once we have established a good baseline for our model.

\section{Building the background dataset}
Some of the object detection pipelines require having a labeled set for binary classification (sign/background). As such, we built a dataset of images containing background (which have no sign in them). We have done so by using the GTSDB\cite{GTSDB} set. It contains 900 images taken from a car's dash camera, each containing at least one traffic sign.

\begin{center}
\includegraphics[width=0.5\linewidth]{background}
\captionof{figure}{An image from the GTSDB dataset containing the Danger sign}
\end{center}

For each image, we run a sliding window (size chosen randomly from a predefined set) and choose a number of samples from the total number of windows generated. The sizes as well as the number of images in the set have to roughly match those in the traffic sign set to ensure balance and scale when doing classification.
\begin{center}
\includegraphics[width=0.6\linewidth]{TrafficSignDimensionsInImages}
\captionof{figure}{Plotting the sizes of the traffic sign images.}
\end{center}
The images seem to be almost of square shape, judging by the plot above. As such, we define seven possible scales that are chosen from when doing sliding window:
$(32,32), (48,48), (64,64), (80,80), (96,96), (112,112), (128,128)$
To match the size of the traffic sign set, we should choose only 45 samples per image (since $\frac{40000}{900}\approx44,44)$. We have chosen a slightly higher number (65) since the dataset has to be curated manually, removing any traffic signs that happen to appear and images that are too similar (pictures of the sky from the same patch, walls, branches of a tree, etc.).
Images 0-700 have been used to generate the training set, and images 701-900 for the test set. After curating the dataset, we end up with aprox. 29000 images in the train foleder and 15000 in the test folder. We generate the validation set by choosing 3000 images from each one and moving it into the validation folder. Here are a few samples of the generated images:\\
\begin{center}
\includegraphics[width=0.6\linewidth]{BackgroundSamples}
\captionof{figure}{Samples taken from the generated background dataset.}
\end{center}


This dataset will be useful when building detection systems such as binary classifiers. In other words, background pictures will be labeled as 0, and images of traffic signs will be labeled as 1. Such a system would be used by running a sliding window over a picture and, for each window (or patch), run the classifier to first determine whether or not it is a sign. If the classifier outputs 1, we run the patch through the convolutional neural network to perform classification as well. Finally, a bounding box is added to the coordinates of the patch containing the sign, annotated with its type. Non-max suppresion and bounding box regression may also be applied.